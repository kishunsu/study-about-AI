{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b0d37628-d11d-43ca-bd70-44567aebbf41"
    }
   },
   "source": [
    "# 評判分析で文章のポジネガを判別しよう\n",
    "\n",
    "機械学習を用いた評判分析における記念碑的論文( http://www.cs.cornell.edu/home/llee/papers/sentiment.pdf )と同様のセットアップで分析を行い、論文の精度を上回れるかチャレンジしてみましょう！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b14e9b2d-e81d-4c46-aaa5-a21ead865efb"
    }
   },
   "source": [
    "## 0.前準備\n",
    "\n",
    "python versionの確認します。<br>\n",
    "jupyter notebookではシェルコマンドの文頭に\"!\"をつけるとそのシェルコマンドをnotebook上で実行することができます。<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.5\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "カレントディレクトリの確認とデータディレクトリの確認をします。<br>\n",
    "osモジュールを使うことでOS依存の機能を使えるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'data', '評判分析入門_advanced.ipynb', '評判分析入門_normal.ipynb']\n"
     ]
    }
   ],
   "source": [
    "print( os.listdir(os.path.normpath(\"./\")) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tokens']\n"
     ]
    }
   ],
   "source": [
    "print( os.listdir(os.path.normpath(\"./data/\")) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.dataの読み込みとモジュールのインポート"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyenvなどを用いているとpandasなどがimportできない場合があります。<br>\n",
    "その可能性の１つとしてlocale（国毎に異なる単位）の設定不足があり得るので、ここではそれを明示的に操作します。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your default locale is None\n",
      "Your locale is set as ja_JP.UTF-8\n"
     ]
    }
   ],
   "source": [
    "def set_locale():\n",
    "    default = os.environ.get('LC_ALL')\n",
    "    print( \"Your default locale is\", default )\n",
    "    if default is None:\n",
    "        os.environ.setdefault('LC_ALL', 'ja_JP.UTF-8')\n",
    "        print( \"Your locale is set as ja_JP.UTF-8\" )\n",
    "\n",
    "set_locale()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回使うデータファイルのパスをpythonのリストとして取得します。<br>\n",
    "globはパス名を見つけたりparseしたりするモジュールです( http://docs.python.jp/3/library/glob.html )。<br>\n",
    "今回扱うデータは https://www.cs.cornell.edu/people/pabo/movie-review-data/ より取得しています。<br>\n",
    "データ構造は下記のようになっています。<br>\n",
    "- data\n",
    "    - README\n",
    "    - tokens\n",
    "        - neg\n",
    "            - file1.txt\n",
    "            - file2.txt\n",
    "            - ...\n",
    "        - pos\n",
    "            - file1.txt\n",
    "            - file2.txt\n",
    "            - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "neg_files = glob.glob( os.path.normpath(\"./data/tokens/neg/*\") )\n",
    "pos_files = glob.glob( os.path.normpath(\"./data/tokens/pos/*\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取得したファイルパスの確認。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/tokens/neg/cv266_tok-5311.txt', 'data/tokens/neg/cv032_tok-9567.txt']\n",
      "['data/tokens/pos/cv048_tok-12726.txt', 'data/tokens/pos/cv018_tok-10094.txt']\n"
     ]
    }
   ],
   "source": [
    "print(neg_files[0:2])\n",
    "print(pos_files[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ読み込みのテストをします。\n",
    "\n",
    "実際に文章を１つ読み込んでみて正しく読み込めているかを確認します。<br>\n",
    "本データは１つのファイルに１文で映画のレビュー文章が記載されています。<br>\n",
    "テキストの読み込みはエンコーディングの問題などでエラーが生じやすいので、慣れるまでは根気強くdebugしましょう。<br>\n",
    "\n",
    "無事に読み込めたら、具体的に１つファイルの中身を読み込んで内容を確認してみましょう。<br>\n",
    "sys はファイルサイズ取得などのシステム上の操作を行うモジュールです( http://docs.python.jp/3/library/sys.html )。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def text_reader(file_path):\n",
    "    python_version = sys.version_info.major\n",
    "    \n",
    "    if python_version >= 3:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                print(line)\n",
    "    else:\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capsule : the much anticipated re-adaptation of the pierre boulle novel comes to the screen as a dark and a little dreary film with lots of chases and fighting , but very little intelligence . visually there is much to like about this version , but the approach is to take an adventure after the style of gulliver's travels and treat it as an action film . that makes it a film without much center . rating : 4 ( 0 to 10 ) , 0 ( -4 to + 4 ) pierre boulle , author of the bridge on the river kwai , wrote planet of the apes ( a . k . a . monkey planet ) , the novel , as a social satire . it reads a lot like a fifth book of gulliver's travels . humans discover a planet in which the roles of apes and humans have been reversed , not unlike the roles of horses and humans on jonathan swift's island of the houyhnhnms . the novel moves somewhat slowly to create some suspense in revealing all the things most film fans know to be true about the nature of the planet . it seems to me there is also a statement about human cruelty to animals , but perhaps i was just looking for that . when rod serling adapted the novel into a film released in 1968 , he added a number of serling touches , familiar from episodes of the twilight zone and changed the ending to make it more serling-ish . the final irony of the original version has become film history . without it there could never have been a \" planet of the apes \" film series . i can surmise only that serling ran into serious script problems in how to handle the tricky question of language . in the book the apes had their own language and the human eventually learned that language . that could have been done in the film , but that would have required the entire film to be subtitled for the non-ape-speaking . serling avoided this by having the apes speak english and , of course , there is some justification for that by the end of the film . justifying why the apes spoke english may have even been the inspiration for his surprise ending . but serling never tackles the all-important question of why a supposedly intelligent human never shows any curiosity or even surprise that the apes speak his own language , a language they had no opportunity to ever hear . few viewers questioned this serious plot hole , however , and the film has become well respected in cinema history . partial credit at least should go to jerry goldsmith whose extremely inventive score is one of goldsmith's best if not his best . when the film's success called for sequels , the filmmakers turned up the violence and they added well-intentioned , though not very subtle , political messages about what was happening in the united states of the 1960s and 1970s . while the first film had a little shooting of guns and what was there seemed a little half- hearted , by the second film , beneath the planet of the apes , there was a good deal more violence and from that point on the series had a lot of violence and chases . the series concluded with battle for the planet of the apes in 1973 . now director tim burton tries his hand at adapting the original book again . for those who thought that the 1968 version was not very faithful to the book , burton's new version is even less faithful . first , he does not really reverse the roles of the humans and the apes . he has them both be intelligent , articulate races battling for a dominance of the planet currently in the hands , uh , make that paws , of the apes . that could be a good story too , but it is not planet of the apes . as with the mission impossible films and so many other cinematic homages to the third quarter of the last century , the title makes promises that the filmmakers have no intention of honoring . in 2029 leo davidson ( mark wahlberg , not this world's most expressive actor ) works on a space station increasing the intelligence and usefulness of apes . then a convenient time storm sweeps him up wizard-of-oz-fashion and drops him on an alien planet . ( yes , he survives this storm , but then no storm is perfect . ) he quickly finds , not greatly to any surprise he shows , that on this planet apes rule and humans drool , but everybody talks . and the language they talk is earth- english . apparently it does not even occur to leo that there is a mystery that needs to be explained about that . the fact it does not occur to leo and apparently didn't occur to tim burton either is the heart of the real horror of this film . both just assumed that if apes were going to talk the language they would speak would be english . in any case having one talking race dominating another makes this not a look at human-animal relationships and more one of the master-slave relationships . outside of sudan and a few other countries this is a less relevant topic . leo is captured to be used as a slave but also is discovered by ari , played by helena bonham carter . ari is an attractive ape with close ties to high political power . she is bent on making the world a better place . perhaps in a previous draft of the script she was called hil-ari . in any case with makeup that stifles her usual pout , carter is just about as attractive as she has ever been in a film . she may want to consider this to become her standard look from this point forward . it is not long before leo escapes with some human and only a couple of sympathetic apes . this is a further abandonment of the source material . the chase severely limits the interplay of ape and human and the examination of each's place in this reversed society , each important in the book . we cannot see how the society works because most of the screentime society has broken down . we see the humans either separated from the apes or fighting them . burton chooses visceral thrills over cerebral ones at almost every turn . this is a miscalculation , as characters so lacking in empathy value are difficult ones to place much emotional investment in . they are basically chess pieces and the viewer has little reason to root for them to win . the 1968 script had little subtlety , with lines like \" i never met an ape i didn't like , \" but at least we cared for what happened to taylor , the main character . most of what this film has to offer is in the visuals . the visual work is spotty but generally nicely done except that so much of the film takes place in the night or in fog . this tends to limit close looks at the makeup . in general it seems much improved from 1968 . the makeup team is led by rick baker instead of john chambers , who did it for the 1968 version . in 1968 chambers makeup was a jaw-dropper . it was realistic enough to almost be believable but flexible enough to show emotion . chambers is good , but if anyone had a chance to best him it would have to be baker . today audiences have higher expectations ; baker's visualization is really an improvement . these visuals work nicely . what does not work is the wire- assisted leaps some of the apes make . they look like they were inspired by the physics- defying leaps of crouching tiger , hidden dragon . apes spring incredible distances . some of the best scenes are apes running into battle looking like they have ape posture , but when they start flying through the air the effect is lost . one final visual problem is that the film frequently shows its budget in what should be spectacular battle scenes the camera shows us only a small group of people close-up . since the days of lon chaney and boris karloff few actors have crossed over to stardom in a role that required heavy make-up . the one actor who has a shot is paul giamatti . it is not that his lines are so good , most are silly jokes . but he delivers them very well . he was always a watchable actor , but has not yet made stardom . as the ape-trader limbo he over-emotes to overcome his ape make-up , but does it very well . in doing so he makes himself the most interesting thing on the screen . he is probably the best thing in the film and conjures up memories of peter ustinov's performance in spartacus . as an in-joke there are several lines in the script borrowed from the 1968 film and an old ape played by charleton heston becomes an allusion to the first film by itself . danny elfman's score has a nice primitive feel , but jerry goldsmith's 1968 tour de force score is a real classic . that score and the whole film will be remembered when the 2001 film is forgotten . i rate the remake 4 on the 0 to 10 scale and a 0 on the -4 to + 4 scale . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_reader(neg_files[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回使うモジュールの情報をまとめておきます。<br>\n",
    "詳細な中身などに関してはご自身で調べてみてください。<br>\n",
    "- matplotlib : グラフなどを描写する<br>\n",
    "http://matplotlib.org/\n",
    "- pandas : dataframeでデータを扱い、集計や統計量算出などがすぐ実行できる<br>\n",
    "http://pandas.pydata.org/\n",
    "- collections : pythonで扱えるデータの型を提供する<br>\n",
    "http://docs.python.jp/3/library/collections.html\n",
    "- numpy : 行列などの数学的オブジェクトを扱う<br>\n",
    "http://www.numpy.org/\n",
    "- sklearn.feature_extraction.DictVectorizer : 辞書データをベクトルの形に変換する<br>\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\n",
    "- sklearnのモデル : SVM, NB, RF<br>\n",
    "http://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
    "- sklearn.grid_search : パラメタの最適な組み合わせ見つける<br>\n",
    "http://scikit-learn.org/stable/modules/grid_search.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/ec2-user/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib # not used in this notebook\n",
    "import pandas as pd # not used in this notebook\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn import svm, naive_bayes\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn import grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.特徴ベクトルの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unigramを作成する関数を定義します。<br>\n",
    "スペース区切りで単語を抽出し、その数をカウントする単純な関数となります。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counter(string):\n",
    "    words = string.strip().split()\n",
    "    count_dict = collections.Counter(words)\n",
    "    return dict(count_dict)\n",
    "\n",
    "def get_unigram(file_path):\n",
    "    result = []\n",
    "    python_version = sys.version_info.major\n",
    "    \n",
    "    if python_version >= 3:\n",
    "        for file in file_path:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    count_dict = word_counter(line)\n",
    "                    result.append(count_dict)\n",
    "    else:\n",
    "        for file in file_path:\n",
    "            with open(file, 'r') as f:\n",
    "                for line in f:\n",
    "                    count_dict = word_counter(line)\n",
    "                    result.append(count_dict)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "関数の挙動を確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': 2,\n",
       " 'am': 1,\n",
       " 'YK.': 1,\n",
       " 'love': 1,\n",
       " 'data': 1,\n",
       " 'analysis': 1,\n",
       " 'using': 1,\n",
       " 'python.': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counter(\"I am YK. I love data analysis using python.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この関数を用いて、negative と positive 両方で unigram を作成します。<br>\n",
    "得られた2つのリストを合わせてモデルのインプット（説明変数）とします。\n",
    "リストの結合は \"+\" で実施できます。 ex.) [1] + [2] = [1,2]<br>\n",
    "negative と positive は各700文ずつありますが、そのうちいくつを使うかをここで指定します。初期設定では全てのデータを使うことになっていますが、後の過程で memory 不足になるようでしたらこの数を減らしてください。<br>\n",
    "\n",
    "また、 jupyter notebook では %% をつけることで magic commands ( https://ipython.org/ipython-doc/3/interactive/magics.html ) という便利なコマンドを実行できます。ここでは処理にかかる時間をセルに表示するコマンドを使用しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 285 ms, sys: 71.4 ms, total: 356 ms\n",
      "Wall time: 1.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "DATA_NUM = 700\n",
    "\n",
    "unigrams_data = get_unigram(neg_files[:DATA_NUM]) + get_unigram(pos_files[:DATA_NUM])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得られたunigram_dataを確認してみます。単語の出現数がカウントされていることが確認できます。<br>\n",
    "合わせてそのデータサイズも確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'actually': 1, ',': 42, \"i'm\": 1, 'fairly': 1, 'sure': 3, 'the': 38, 'experience': 2, 'of': 20, 'having': 3, 'my': 4, 'flesh': 1, 'torn': 1, 'and': 17, 'mutilated': 1, 'by': 2, 'barbed': 1, 'wire': 7, 'would': 2, 'have': 4, 'been': 1, 'more': 5, 'positive': 1, 'than': 5, 'watching': 1, 'this': 5, 'movie': 8, '.': 45, '\"': 16, 'baywatch': 1, 'babe': 3, 'pamela': 8, 'anderson': 3, 'lee': 1, 'proves': 1, 'once': 2, 'for': 6, 'all': 3, 'that': 7, 'she': 8, 'should': 1, 'keep': 1, 'her': 15, \"double-d's\": 1, 'on': 4, 'small': 1, 'screen': 2, 'at': 3, 'least': 1, 'there': 4, 'you': 5, \"don't\": 3, 'to': 14, 'pay': 3, 'see': 2, 'cleavage': 2, 'those': 3, 'viewers': 1, 'out': 4, 'who': 4, 'only': 5, 'lay': 1, 'down': 2, 'money': 2, 'in': 14, 'hopes': 1, 'seeing': 2, 'pam': 3, 'topless': 2, 'hate': 1, 'burst': 1, 'your': 1, 'bubble': 1, 'but': 7, 'are': 2, 'no': 2, 'full-fledged': 1, 'nude': 3, 'scenes': 4, 'barb': 6, \"wouldn't\": 2, 'be': 3, 'reading': 1, 'review': 1, 'right': 2, 'now': 1, 'if': 6, 'i': 9, 'had': 1, 'known': 1, 'fact': 2, 'going': 3, \"can't\": 2, 'go': 1, 'back': 1, 'time': 1, 'reverse': 1, 'mistake': 1, 'can': 1, 'warn': 1, 'other': 1, 'horny': 1, 'teenage': 1, 'boys': 1, 'they': 3, 'do': 2, 'tease': 3, 'us': 2, 'a': 26, 'few': 1, 'times': 2, 'with': 1, 'where': 1, 'almost': 1, 'is': 13, \"it's\": 6, 'so': 1, 'quick': 1, 'even': 4, 'tell': 2, 'whether': 1, \"you're\": 1, 'andersons': 1, 'or': 3, 'not': 2, 'nipple': 1, 'hallucination': 1, 'sort': 1, 'thing': 1, 'was': 2, 'adapted': 1, 'from': 1, 'comic': 2, 'book': 2, 'interesting': 1, 'because': 2, 'probably': 2, 'woman': 4, 'looks': 1, 'like': 4, 'character': 2, 'real': 1, 'life': 2, 'wearing': 1, 'low-cut': 1, 'leather': 1, 'office': 1, 'work': 1, '(': 7, 'businesswoman': 1, 'bondage': 1, 'wear': 1, ')': 7, 'unreal': 1, 'body': 1, 'proportions': 1, 'yes': 1, 'mona': 1, 'lisa': 1, 'man-made': 1, 'beauty': 2, 'senses': 1, 'one': 4, 'definite': 1, 'though': 1, 'looking': 1, 'never': 3, 'an': 3, 'unpleasant': 1, \"you'd\": 1, 'just': 2, 'think': 3, 'began': 1, 'career': 1, 'as': 4, 'playboy': 1, 'playmate': 1, 'any': 2, 'reservations': 1, 'about': 3, 'appearing': 1, 'obvious': 1, \"wasn't\": 1, 'pulling': 1, 'sharon': 1, 'stone': 1, 'trying': 1, 'make': 1, 'people': 1, 'attention': 1, 'acting': 1, 'skills': 1, 'mean': 1, 'shows': 1, 'off': 1, 'here': 1, \"jeweler's\": 1, 'convention': 1, 'set': 1, 'year': 4, '2017': 1, 'worst': 2, 'says': 1, 'say': 1, 'america': 1, 'through': 2, 'second': 1, 'civil': 1, 'war': 1, 'nightclub': 1, 'owner': 1, 'free': 2, 'city': 1, 'nation': 1, 'silicone': 1, 'valley': 1, 'also': 1, 'hires': 1, 'herself': 1, 'bounty': 1, 'hunter': 1, 'when': 1, 'price': 1, 'posing': 1, 'first': 1, 'stripper': 1, 'later': 1, 'prostitute': 1, 'call': 1, 'hates': 1, 'reminds': 2, 'way': 1, 'too': 1, 'many': 1, 'imagine': 1, 'does': 1, 'trapeze': 1, 'strip': 2, 'bar': 1, 'while': 2, 'hose': 1, 'sprayed': 1, 'being': 1, 'referred': 1, 'such': 2, 'sexist': 1, 'demeaning': 1, 'term': 1, 'liberated': 1, \"shouldn't\": 1, 'hear': 1, 'words': 4, 'during': 1, 'especially': 1, 'since': 1, 'talking': 1, 'pig': 1, 'success': 1, 'plot': 1, 'ha-ha': 1, 'revolves': 1, 'around': 1, 'pair': 1, 'contact': 1, 'lenses': 1, 'allow': 1, 'their': 2, 'wearer': 1, 'pass': 1, 'congressional': 2, \"directorate's\": 1, 'retina': 1, 'scanners': 1, 'characters': 1, \"they're\": 1, 'meets': 1, 'eye': 1, 'me': 1, 'transformers': 1, 'cartoon': 1, 'wished': 1, 'could': 2, 'somehow': 2, 'transformed': 1, 'into': 1, 'something': 1, 'decent': 1, 'happened': 1, 'continued': 1, 'its': 1, 'path': 1, 'lame': 1, 'action': 3, 'starring': 1, 'van-damme': 1, 'big-busted': 1, 'kickboxer': 1, 'resistance': 1, 'accomplices': 1, 'ex-boyfriend': 1, 'axel': 1, 'tamuera': 1, 'morrison': 1, 'cora': 2, 'victoria': 1, 'rowell': 1, 'thank': 1, 'god': 1, \"didn't\": 2, 'name': 2, 'reef': 1, 'bad': 2, 'enough': 1, 'originally': 1, \"doesn't\": 1, 'take': 1, 'sides': 1, 'giving': 1, 'some': 1, 'speech': 1, \"she's\": 1, 'loyal': 1, 'changes': 1, 'mind': 1, 'bastards': 1, 'kill': 1, 'blind': 1, 'brother': 1, 'jack': 1, 'noseworthy': 2, 'bon': 1, 'jovi': 1, 'always': 1, 'video': 1, 'fame': 1, 'still': 1, 'know': 2, 'step': 2, 'up': 4, 'he': 2, 'definitely': 1, 'nose': 1, 'worthy': 2, \"isn't\": 1, 'sponge': 1, 'gets': 1, 'ready': 1, 'avenge': 1, 'his': 1, 'death': 2, 'grabbing': 1, 'armful': 1, 'semi-automatic': 1, 'weapons': 1, 'strapping': 1, 'ammunitions': 1, 'belt': 1, 'chest': 1, 'rambo': 1, 'bimbo': 1, '!': 1, 'mark': 1, 'will': 1, 'over': 2, 'cinemax': 1, 'network': 1, 'got': 2, 'elements': 2, 'direct-to-video': 1, 'releases': 1, 'featured': 1, \"hbo's\": 1, 'bastard': 1, 'cousin': 1, 'cable': 1, 'channel': 1, \"i'd\": 1, 'watch': 1, 'it': 2, 'come': 1, 'non-titillating': 1, 'voyeurism': 1, 'laughable': 1, 'flashbacks': 1, 'dialogue': 1, 'cliches': 1, 'wazzoo': 1, \"there's\": 1, 'narrator': 1, 'beginning': 1, 'setting': 1, \"movie's\": 1, 'premise': 1, 'scroll': 1, 'someone': 1, 'needs': 1, 'wookie': 1, \"ain't\": 1, 'star': 1, 'wars': 1, \"you've\": 1, 'seen': 1, 'flick': 1, 'past': 1, 'fifteen': 1, 'years': 1, \"you'll\": 1, 'recognize': 1, 'plenty': 1, 'lifted': 1, 'has': 2, 'obligatory': 1, 'trucks': 1, 'flipping': 1, 'car': 1, 'crashes': 1, 'explosions': 1, 'broken': 1, 'glass': 1, 'slow-motion': 1, 'shots': 1, 'bodies': 1, 'falling': 1, 'hundreds': 1, 'feet': 1, 'automatic-pilot': 1, 'movies': 1, 'anyone': 1, 'write': 1, 'direct': 1, 'two': 2, 'things': 2, 'what': 1}\n",
      "data size : 0.011264 [MB]\n"
     ]
    }
   ],
   "source": [
    "print( unigrams_data[0] )\n",
    "print( \"data size :\", sys.getsizeof(unigrams_data) / 1000000, \"[MB]\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上で得られたデータは unigram という 1400 の要素を持つリストであり、各要素は key と value からなる辞書となっています。<br>\n",
    "これを扱いやすい行列の形にします。<br>\n",
    "ここでは各行が１つのレビューテキストに対応するようにして、各列が単語、要素がその単語の出現数というデータを作成します。<br>\n",
    "scikit-learn で実装されている DictVectorizer という関数を使うことでそれが簡単に実行できます。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 567 ms, sys: 0 ns, total: 567 ms\n",
      "Wall time: 569 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vec = DictVectorizer()\n",
    "feature_vectors_csr = vec.fit_transform( unigrams_data )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作成したデータを確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1400x44219 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 496525 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vectors_csr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "行列の全成分（行成分×列成分）は 60,000,000 要素くらいありますが、このうちのほとんどは 0 で 0 以外の値が入っているのは500,000程度です。<br>\n",
    "この CSR(Compressed Sparse Row) matrix というのはこのような疎行列をの 0 でない成分だけを保持する賢いものになっています。<br>\n",
    "\n",
    "一方で 0 の成分を陽に保って普通の行列としてデータを保持することも可能です。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimension : (1400, 44219)\n",
      "[ 0.  1. 16. ...  0.  0.  0.]\n",
      "data size : 495.252912 [MB]\n"
     ]
    }
   ],
   "source": [
    "feature_vectors = vec.fit_transform( unigrams_data ).toarray()\n",
    "print( \"data dimension :\", feature_vectors.shape )\n",
    "print( feature_vectors[0] )\n",
    "print( \"data size :\", sys.getsizeof(feature_vectors) / 1000000, \"[MB]\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こちらはデータが非常に大きくなっていますが、これは 0 という成分を陽に保持しているためです。<br>\n",
    "この段階で memory error が生じる場合は一度 kernel を restart して DATA_NUM の数を減らして再実行してください。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.ラベルデータの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回扱うデータセットは全てに negative, positive というラベルが振られています。<br>\n",
    "ここではそのラベルを neagtive → 0, neagtive → 1 とすることで二値判別問題のセットアップを構築します。<br>\n",
    "先ほど作った説明変数となる特徴ベクトルはnegative sample 700文とpositive sample 700文を縦につなげて作ったものなので、0が700個と1が700個並んでいるベクトルを作成すれば必要なラベルを作れます。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.r_[np.tile(0, DATA_NUM), np.tile(1, DATA_NUM)] #二つの配列を結合している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正しい位置で0と1の振替がなされているか確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 1 1\n"
     ]
    }
   ],
   "source": [
    "print( labels[0], labels[DATA_NUM-1], labels[DATA_NUM], labels[2*DATA_NUM-1]  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.学習用データとテスト用データの作成方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "論文の記述によれば、データを偏りがないように3分割に分け、 three fold cross validation でモデルを評価しています。<br>\n",
    "ここでは乱数を生成して、データを3等分することで同様の状況を再現することにします。<br>\n",
    "結果の再現性を担保するために乱数の seed も設定しておきます。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7789)\n",
    "\n",
    "shuffle_order = np.random.choice( 2*DATA_NUM, 2*DATA_NUM, replace=False )#falseは重複なしで2*DATA_NUMから2*DATA_NUMを抽出すること"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成した乱数の中身を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length : 1400\n",
      "first 10 elements : [1235 1232  910  162  343 1160  221  545 1112 1322]\n"
     ]
    }
   ],
   "source": [
    "print( \"length :\", len(shuffle_order) )\n",
    "print( \"first 10 elements :\", shuffle_order[0:10] )#shuffle_orderの中にindex番号を格納する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分割したデータセットに含まれるラベル=1の数を数えることでデータの偏りが生じていないかを確認します。<br>\n",
    "明らかに偏りが生じてしまった場合は乱数のseedを設定し直します。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one third of the length : 466\n",
      "# of '1' in 1st set : 227\n",
      "# of '1' in 2nd set : 233\n",
      "# of '1' in 3rd set : 240\n"
     ]
    }
   ],
   "source": [
    "one_third_size = int( 2*DATA_NUM / 3. )\n",
    "print( \"one third of the length :\", one_third_size )\n",
    "\n",
    "print( \"# of '1' in 1st set :\", np.sum( labels[ shuffle_order[:one_third_size] ]  ) )\n",
    "print( \"# of '1' in 2nd set :\", np.sum( labels[ shuffle_order[one_third_size:2*one_third_size] ]  ) )\n",
    "print( \"# of '1' in 3rd set :\", np.sum( labels[ shuffle_order[2*one_third_size:] ]  ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.モデルを学習して精度を検証"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習に必要な関数を定義します。<br>\n",
    "ここではモデルとして{Support Vector Machine(SVM), Naive Bayes(NB), Random Forest(RF)}を用います。<br>\n",
    "モデルの性能測定は予測と答えが一致する数をカウントして正答率を求めることで実施します。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与えられたリストをN分割する関数を定義します。<br>\n",
    "割り切れない場合はうしろのリストに格納します。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_splitter(seq, N):\n",
    "    avg = len(seq) / float(N)\n",
    "    out = []\n",
    "    last = 0.0\n",
    "    \n",
    "    while last < len(seq):\n",
    "        out.append( seq[int(last):int(last + avg)] )\n",
    "        last += avg\n",
    "        \n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "望ましい動作をするか確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([range(0, 4), range(4, 9), range(9, 14)], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_splitter(range(14), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの学習や予測のための関数を定義します。<br>\n",
    "- train_model : 説明変数とラベルと手法を与えることでモデルを学習する\n",
    "- predict : モデルと説明変数を与えることでラベルを予測する\n",
    "- evaluate_model : 予測したラベルと実際の答えの合致数を調べる\n",
    "- cross_validate : cross_validationを実行する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(features, labels, method='SVM', parameters=None):\n",
    "    ### set the model\n",
    "    if method == 'SVM':\n",
    "        model = svm.SVC()\n",
    "    elif method == 'NB':\n",
    "        model = naive_bayes.GaussianNB()\n",
    "    elif method == 'RF':\n",
    "        model = RandomForestClassifier()\n",
    "    else:\n",
    "        print(\"Set method as SVM (for Support vector machine), NB (for Naive Bayes) or RF (Random Forest)\")\n",
    "    ### set parameters if exists\n",
    "    if parameters:\n",
    "        model.set_params(**parameters)\n",
    "    ### train the model\n",
    "    model.fit( features, labels )\n",
    "    ### return the trained model\n",
    "    return model\n",
    "\n",
    "def predict(model, features):\n",
    "    predictions = model.predict( features )\n",
    "    return predictions\n",
    "\n",
    "def evaluate_model(predictions, labels):\n",
    "    data_num = len(labels)\n",
    "    correct_num = np.sum( predictions == labels )##predictionsとlabelsが等しいものを取り出し,その要素の和をとることで予測と実際があっていた数を算出している\n",
    "    return data_num, correct_num\n",
    "\n",
    "def cross_validate(n_folds, feature_vectors, labels, shuffle_order, method='SVM', parameters=None):\n",
    "    result_test_num = []\n",
    "    result_correct_num = []\n",
    "    \n",
    "    n_splits = N_splitter( range(2*DATA_NUM), n_folds )\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        print( \"Executing {0}th set...\".format(i+1) )\n",
    "        \n",
    "        test_elems = shuffle_order[ n_splits[i] ]#cross validationでテスト用の要素を格納する。\n",
    "        train_elems = np.array([])\n",
    "        train_set = n_splits[ np.arange(n_folds) !=i ]#\n",
    "        for j in train_set:\n",
    "            train_elems = np.r_[ train_elems, shuffle_order[j] ]\n",
    "        train_elems = train_elems.astype(np.integer)\n",
    "\n",
    "        # train\n",
    "        model = train_model( feature_vectors[train_elems], labels[train_elems], method, parameters )\n",
    "        # predict\n",
    "        predictions = predict( model, feature_vectors[test_elems] )\n",
    "        # evaluate\n",
    "        test_num, correct_num = evaluate_model( predictions, labels[test_elems] )\n",
    "        result_test_num.append( test_num )\n",
    "        result_correct_num.append( correct_num )\n",
    "    \n",
    "    return result_test_num, result_correct_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記関数はCross validationの数を変数として設定できるように作ってあります。<br>\n",
    "今回は上述のように 3-folds で分析を行います。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "準備ができたのでここでモデルを学習してその精度を確認してみましょう。<br>\n",
    "とりあえず何も考えずに準備した Bag Of Words をインプットにしてモデルを学習し、その精度を確認してみます。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing 1th set...\n",
      "Executing 2th set...\n",
      "Executing 3th set...\n",
      "CPU times: user 12.2 s, sys: 0 ns, total: 12.2 s\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ans,corr = cross_validate(N_FOLDS, feature_vectors_csr, labels, shuffle_order, method='SVM', parameters=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision :  62.4 %\n"
     ]
    }
   ],
   "source": [
    "print( \"average precision : \", np.around( 100.*sum(corr)/sum(ans), decimals=1 ), \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際にどのような文章では予測が正解してどのような文章では予測を外しているのかを見てみます。<br>\n",
    "モデルを学習して予測を行い、データを見てみます。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.89 s, sys: 0 ns, total: 2.89 s\n",
      "Wall time: 2.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "svm_model = train_model(\n",
    "    features=feature_vectors_csr[shuffle_order[0:950],:]\n",
    "    , labels=labels[shuffle_order[0:950]]\n",
    "    , method='SVM'\n",
    "    , parameters=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data :  [ 490 1276  794  892  504  463   48  289 1218 1137] correct label :  [0 1 1 1 0 0 0 0 1 1]\n",
      "predict label :  [0 1 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"data : \" ,shuffle_order[970:980], \"correct label : \", labels[shuffle_order[970:980]])\n",
    "print( \"predict label : \", predict(svm_model, feature_vectors_csr[shuffle_order[970:980], :]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "予測が間違っているものを見てみます。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a pseudo-intellectual film about the pseudo-intellectual world of art magazines , high art is as wasted as its drug-addled protagonists . in the only notable part of the movie , ally sheedy and radha mitchell deliver nice performances in the two leading roles , not that lisa cholodenko's script or direction makes you care much about either character . living in a world of heroin induced highs , they float along until they fall in love with each other . this uninviting picture , full of pretentious minor characters , has a receptionist that reads dostoevski and a woman in the restroom line who is a certified genius , having recently been awarded a prestigious mcarthur grant . 24-year-old syd ( radha mitchell ) , who has a rather bland , live-in boyfriend , was just promoted to assistant editor at the artistic photography magazine \" frame . \" although the receptionist is impressed , syd is mainly a gofer for her boss until she meets famous photographer lucy berliner ( ally sheedy ) . for her to do photos for \" frame , \" lucy demands that syd be promoted to editor and assigned to her since lucy fancies her . lucy lives with her current lover , a washed up german actress named greta , played with a frequently indecipherable series of mumbles by patricia clarkson . the two of them and their friends wile away their time snorting and shooting up dope , usually heroin . this does not happen in a single episode , but becomes more commonplace than sleeping in the picture . syd , who lives in the apartment below them , joins in on the fun and becomes a member of the zombie club . lucy seems pretty happy with her life of drugs , which apparently is funded by her mother . lucy quit working professionally 10 years ago since she thought she was being \" pigeonholed , \" and , since her mother has money , we can only assume that that's how lucy supports her habit and procures her living expenses . a typical scene has the editors arguing about whether a potential photographer's work is transcendental or merely classical . that no one has a clue as to the dogma they are spouting becomes obvious but not particularly funny . \" your work has a cultural currency that is important now , \" is the artist-speak that the frame's manager uses to convince lucy to show her pictures in the magazine . when the big scene comes in which lucy puts the moves on syd , her idea of a romantic line is , \" i want to get high with you . \" in lucy's world , sex and drugs come hand-in-hand . and the movie , except for the obligatory scene of someone almost overdosing , shows drug usage as being a hip and natural part of the art scene . this vacuous picture throws in a standard downer ending in an attempt to manipulate our emotions . in another movie , it might have worked , but in this one the reaction is likely to be decidedly muted . high art runs 1 : 36 . it is rated r for explicit sex , pervasive drug use and language and is not appropriate for those younger than college age . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_reader(neg_files[490-1])#なぜこれが予測が間違っているものとわかった？？テキストだと490が1と予測されているから"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "長いので全部読むのは大変ですが、読んでみると確かに negative なレビューだとわかります。<br>\n",
    "一方で単語だけ見ると　feel-good や happy など positive と捉えられるような単語も出現しています。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "予測が当たっているものも見てみます。<br>\n",
    "ファイルの指定方法に注意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in these days of overlong movies ( meet joe black , the thin red line , the mask of zorro ) it is a shame that films like waking ned devine can't be longer than a paltry 90 minutes . this is just a cute movie , even through its mildly risque subject matter . old friends jackie ( bannen ) and michael ( kelley ) try to find the lottery winner ( they deduce must live in their dinky town of about 60 ) so that they might kiss up to him and share the winnings . through process of elimination , they find that it must be lovable old ned devine , who they find sitting in front of his tv , clutching the winning lottery ticket in his cold dead hand . what results is thuroughly amusing , as jackie tries to convince his wife that not claiming it would be wrong , and that they could really benefit . after all , old ned won't miss it . rather than divulge the later twists and turns , i'll stop here merely pointing out that jackie and michael get into all sorts of trouble in their little sleepy irish villiage . bannen and kelly are a perfect pair . one slightly stout , the other as thin as a rail . both getting on in years , they make such a cute pair of old codgers . waking ned devine may even be seen as a \" full monty \" for the geriatric set , especially since kelly gets buff-o for one amusing scene . waking ned devine is by no means perfect , but it is so sincere and touching that it looks so much better than most films . the performances by everyone in the town are great , particularly the two leads . there is one twist at the end which i find unnecessary , but it hardly ruins the picture . writer/director kirk jones should be held up as an example to all those hollywood screenwriters . scripts as creative and endearing as this should be the norm , not the exception . perhaps it makes us appreciate this wonderful film even more . had i held off on my year's best/worst list for another day , waking ned devine ( officially released in late november of 98 ) surely would have graced the short group of the year's finest films . it is light , but thought provoking and sweet . i can't think of anyone who shouldn't see ( or wouldn't enjoy ) this film . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_reader(pos_files[1276-700-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後の一文などで確かに positive な評価をしているレビューだということが見て取れます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.パラメタチューニング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grid searchでパラメタをチューニングすることで、どの程度精度が上がるかを確認してみます。<br>\n",
    "scikit-learnにはgrid searchが実装されているので、ここではそれを用いてパラメタをチューニングしてみます。<br>\n",
    "grid search に関しては http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html など。<br>\n",
    "下記セルの実行には4,5分程度かかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 36s, sys: 2.85 s, total: 4min 39s\n",
      "Wall time: 4min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "search_parameters = [\n",
    "    {'kernel': ['rbf'], 'gamma': [1e-2, 1e-3, 1e-4], 'C': [0.1, 1, 10, 100, 1000]},\n",
    "    {'kernel': ['linear'], 'C': [0.1, 1, 10, 100, 1000]}\n",
    "]\n",
    "#search_parametersの中身について？？？\n",
    "model = svm.SVC()\n",
    "clf = grid_search.GridSearchCV(model, search_parameters)\n",
    "clf.fit( feature_vectors_csr, labels )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grid searchによって発見したパラメタやスコアを確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best paremters :  {'gamma': 0.0001, 'C': 100, 'kernel': 'rbf'}\n",
      "best scores :  0.797857142857\n"
     ]
    }
   ],
   "source": [
    "print(\"best paremters : \", clf.best_params_)\n",
    "print(\"best scores : \", clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing 1th set...\n",
      "Executing 2th set...\n",
      "Executing 3th set...\n",
      "CPU times: user 13.3 s, sys: 113 ms, total: 13.5 s\n",
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ans,corr = cross_validate(N_FOLDS, feature_vectors_csr, labels, shuffle_order, method='SVM', parameters=clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision :  79.5 %\n"
     ]
    }
   ],
   "source": [
    "print( \"average precision : \", np.around( 100.*sum(corr)/sum(ans), decimals=1 ), \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "精度が15%程度も向上しました！機械学習においてモデルのパラメタチューニングが非常に重要であることが伺えます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.簡単な特徴量変換による効果の確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "論文に記載してあるように、Bag of Words(BoW) のカウント数を全て1にしてみることで精度にどのような変化が生じるかを調べてみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors_csr.data[ feature_vectors_csr.data > 0 ] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変換したデータを用いて同様に学習プロセスを実行してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing 1th set...\n",
      "Executing 2th set...\n",
      "Executing 3th set...\n",
      "CPU times: user 14.4 s, sys: 118 ms, total: 14.6 s\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ans, corr = cross_validate(N_FOLDS, feature_vectors_csr, labels, shuffle_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision :  49.1 %\n"
     ]
    }
   ],
   "source": [
    "print( \"average precision : \", np.around( 100.*sum(corr)/sum(ans), decimals=1 ), \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なんと精度がだいぶ落ちてしまいました。。。<br>\n",
    "しかも現在の問題設定は 0 か 1 を判別するものなので、ランダムに判別するモデルを作ったとしても 50% 程度になります。<br>\n",
    "それと同程度ということは、そもそもモデルの学習が上手くいっていないのではないかということが疑われます。<br>\n",
    "そのことを検証してみるために、もう一度パラメタチューニングを実施してみます。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 40s, sys: 2.05 s, total: 4min 42s\n",
      "Wall time: 4min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "search_parameters = [\n",
    "    {'kernel': ['rbf'], 'gamma': [1e-2, 1e-3, 1e-4], 'C': [0.1, 1, 10, 100, 1000]},\n",
    "    {'kernel': ['linear'], 'C': [0.1, 1, 10, 100, 1000]}\n",
    "]\n",
    "\n",
    "model = svm.SVC()\n",
    "clf = grid_search.GridSearchCV(model, search_parameters)\n",
    "clf.fit( feature_vectors_csr, labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best paremters :  {'gamma': 0.001, 'C': 10, 'kernel': 'rbf'}\n",
      "best scores :  0.814285714286\n"
     ]
    }
   ],
   "source": [
    "print(\"best paremters : \", clf.best_params_)\n",
    "print(\"best scores : \", clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing 1th set...\n",
      "Executing 2th set...\n",
      "Executing 3th set...\n",
      "CPU times: user 14.1 s, sys: 107 ms, total: 14.2 s\n",
      "Wall time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ans, corr = cross_validate(N_FOLDS, feature_vectors_csr, labels, shuffle_order, method='SVM', parameters=clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision :  82.7 %\n"
     ]
    }
   ],
   "source": [
    "print( \"average precision : \", np.around( 100.*sum(corr)/sum(ans), decimals=1 ), \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメタを調整することで高い精度を発揮することが分かりました！<br>\n",
    "この精度は論文に記載されているもの(82.9%)とほぼ同程度のものとなっています。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.SVM以外のモデルを実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes は sparse matrix 型のインプットを受け付けないので、numpy arrayとして作成したデータを入れなければなりません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing 1th set...\n",
      "Executing 2th set...\n",
      "Executing 3th set...\n",
      "average precision :  62.3 %\n",
      "CPU times: user 3.61 s, sys: 3.41 s, total: 7.02 s\n",
      "Wall time: 7.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ans, corr = cross_validate(N_FOLDS, feature_vectors, labels, shuffle_order, method='NB')\n",
    "print( \"average precision : \", np.around( 100.*sum(corr)/sum(ans), decimals=1 ), \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest も実行してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing 1th set...\n",
      "Executing 2th set...\n",
      "Executing 3th set...\n",
      "average precision :  64.9 %\n",
      "CPU times: user 2.37 s, sys: 841 ms, total: 3.22 s\n",
      "Wall time: 3.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ans, corr = cross_validate(N_FOLDS, feature_vectors, labels, shuffle_order, method='RF')\n",
    "print( \"average precision : \", np.around( 100.*sum(corr)/sum(ans), decimals=1 ), \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gaussianNBはチューニング可能なパラメタはありませんが、RFはパラメタが多いため、興味がある方は下のセルのコメントアウトを外して、パラメタによって結果がどう変わるかを調べてみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# search_parameters = {\n",
    "#     \"max_depth\": [3, 5, None],\n",
    "#     \"max_features\": [1000,5000,10000,20000,30000],\n",
    "#     \"min_samples_split\": [1,2,3,4,5,10],\n",
    "#     \"min_samples_leaf\": [5,10,20,50],\n",
    "#     \"bootstrap\": [True, False],\n",
    "#     \"criterion\": [\"gini\", \"entropy\"]\n",
    "# }\n",
    "\n",
    "# model = RandomForestClassifier()\n",
    "# clf = grid_search.GridSearchCV(model, search_parameters)\n",
    "# clf.fit( feature_vectors_csr, labels )\n",
    "\n",
    "# print(clf.best_params_)\n",
    "\n",
    "# ans, corr = cross_validate(N_FOLDS, feature_vectors_csr, labels, shuffle_order, method='RF', parameters=clf.best_params_)\n",
    "# print( \"average precision : \", np.around( 100*sum(corr)/sum(ans), decimals=1 ), \"%\" )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nbpresent": {
   "slides": {},
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
