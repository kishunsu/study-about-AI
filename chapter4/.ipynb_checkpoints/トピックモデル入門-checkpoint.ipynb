{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "from __future__ import print_function\n",
    "\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "import lda\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>データを読み込もう</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Documents:  1000\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.normpath('document_word_data.json'), 'r') as f:\n",
    "    doc_data = json.load(f)\n",
    "\n",
    "all_doc_index = doc_data.keys()\n",
    "print('Total Documents: ', len(all_doc_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Windowsの方でutf-8のテキストファイルが開けない場合は、こちらを実行してみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Documents:  1000\n"
     ]
    }
   ],
   "source": [
    "# Windowsの方で上記のコードが失敗する場合\n",
    "with codecs.open(os.path.normpath('document_word_data.json'), 'r', \"utf-8\") as f:\n",
    "    doc_data = json.load(f)\n",
    "\n",
    "all_doc_index = doc_data.keys()\n",
    "print('Total Documents: ', len(all_doc_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この読み込んだjsonデータはこんな感じです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ママ,子供,健康,づくり,新た,ライフスタイル,提案,ママ,マルシェ,府,府,市,さまざま,家族,ら,日,もん,商品,ほか,ハロ,子供,商品,プレゼント,各日,人,会場,木,ぬくもり,木,子供,づくり,会,木,日,午後,時,今年度,森林,林業,木材,大使,ミス,日本,みどり,帆,南,さん,府,木材,会,湯川,昌子,さん,ら,女性,人,参加,スギ,ヒノキ,木材,放出,健康,効果,木材,こと,女性,入場,無料,文化,園,入園,料,大人,円,円'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "','.join(doc_data['715'])\n",
    "#print(doc_data['715'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語のインデックスを作るために、全ての単語のリストを作ります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocablary Number:  8709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vocab = []\n",
    "for doc_idx in all_doc_index:\n",
    "    all_vocab += doc_data[doc_idx]\n",
    "\n",
    "# 重複を消すためにsetしてlistにする\n",
    "all_vocab = list(set(all_vocab))\n",
    "vocab_num = len(all_vocab)\n",
    "print('Vocablary Number: ', vocab_num)\n",
    "type(all_doc_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#スライシングして学習データとテスト用データに分けるため、リストをNumpyの配列にしておきます。\n",
    "#all_doc_indexは辞書型の辞書型のkeyである\n",
    "all_doc_index_ar = np.array(list(all_doc_index))\n",
    "\n",
    "train_portion = 0.7\n",
    "train_num = int(len(all_doc_index_ar) * train_portion)\n",
    "\n",
    "np.random.shuffle(all_doc_index_ar)\n",
    "train_doc_index = all_doc_index_ar[:train_num]\n",
    "test_doc_index = all_doc_index_ar[train_num:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先にからっぽのスパース行列を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_train = sparse.lil_matrix((len(train_doc_index), len(all_vocab)),\n",
    "                            dtype=np.int)\n",
    "A_test = sparse.lil_matrix((len(test_doc_index), len(all_vocab)),\n",
    "                           dtype=np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ListからNumpyのArrayに直します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocab_ar = np.array(all_vocab)\n",
    "train_doc_index_ar = np.array(train_doc_index)\n",
    "test_doc_index_ar = np.array(test_doc_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スパース行列に成分を入れていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train total elements num : 34294\n",
      "8473\n",
      "Test total elements num : 13750\n"
     ]
    }
   ],
   "source": [
    "# 学習用\n",
    "#Counterを冒頭でimportしている\n",
    "#リストに格納されている物がkey,登場回数がvalueに格納される\n",
    "train_total_elements_num = 0\n",
    "for i in range(len(train_doc_index)):\n",
    "    doc_idx = train_doc_index[i]\n",
    "    row_data = Counter(doc_data[doc_idx])\n",
    "    \n",
    "    for word in row_data.keys():\n",
    "        #all_vocab_ar == wordとなるwordの行番号をword_idxに格納している\n",
    "        word_idx = np.where(all_vocab_ar == word)[0][0]\n",
    "        #単語がいくつ出たかは列に格納するためword_idxを列に入れる\n",
    "        A_train[i, word_idx] = row_data[word]\n",
    "        train_total_elements_num += 1\n",
    "print('Train total elements num :', train_total_elements_num)\n",
    "print(word_idx)\n",
    "\n",
    "# テスト用\n",
    "test_total_elements_num = 0\n",
    "for i in range(len(test_doc_index)):\n",
    "    doc_idx = test_doc_index[i]\n",
    "    row_data = Counter(doc_data[doc_idx])\n",
    "    \n",
    "    for word in row_data.keys():\n",
    "        word_idx = np.where(all_vocab_ar == word)[0][0]\n",
    "        A_test[i, word_idx] = row_data[word]\n",
    "        test_total_elements_num += 1\n",
    "print('Test total elements num :', test_total_elements_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  CountVectorizerを用いてdoc_dataから簡単に疎行列を作ることもできます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=lambda a: a, analyzer=lambda a: a)\n",
    "vectorizer.fit(doc_data[idx] for idx in all_doc_index)\n",
    "A_train = vectorizer.transform(doc_data[idx] for idx in train_doc_index)\n",
    "A_test = vectorizer.transform(doc_data[idx] for idx in test_doc_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実際にLDAを適用してみよう (Scikit-learnを使った例）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LatentDirichletAllocation(n_topics=20,\n",
    "                                   doc_topic_prior=0.001,\n",
    "                                   topic_word_prior=0.5,\n",
    "                                   max_iter=5,\n",
    "                                   learning_method='online',\n",
    "                                   learning_offset=50.,\n",
    "                                   random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.62874734 0.60120207 0.63093741 ... 0.60698304 0.58610315 0.60191257]\n",
      " [0.55997351 0.59715763 0.62227582 ... 0.6055909  0.59843568 0.61186333]\n",
      " [0.57264999 0.66155909 0.60704807 ... 0.59379809 0.61550943 0.61804448]\n",
      " ...\n",
      " [0.60398376 0.83305737 0.63075313 ... 0.59737468 0.57740033 0.60241614]\n",
      " [0.55302325 1.08771899 0.58169826 ... 0.57208699 1.24035172 0.64197292]\n",
      " [0.58349383 0.59543234 0.58241523 ... 0.60735426 0.59018344 0.60214521]]\n"
     ]
    }
   ],
   "source": [
    "model1.fit(A_train)\n",
    "print(model1.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずトピック x 単語を見てみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05032118 0.0471789  0.04426835 ... 0.05092908 0.04633547 0.02282043]\n",
      " [0.04481694 0.04686152 0.04366063 ... 0.05081227 0.04731044 0.0231977 ]\n",
      " [0.04583149 0.05191538 0.0425922  ... 0.04982279 0.04866024 0.02343204]\n",
      " ...\n",
      " [0.04833926 0.06537358 0.04425542 ... 0.05012289 0.04564746 0.02283952]\n",
      " [0.04426068 0.08535797 0.04081359 ... 0.04800112 0.09805831 0.02433925]\n",
      " [0.04669936 0.04672613 0.04086389 ... 0.05096023 0.04665805 0.02282925]]\n"
     ]
    }
   ],
   "source": [
    "normalize_components = model1.components_ / model1.components_.sum(axis=0)\n",
    "print(normalize_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "インスタグラムハッシュタグ ジグ 楽 彷彿 エルドレッド 判明 本年 パズドラ ウェルカム ゼブラ 康則 武家 有名 横 冷水 ヒルトン てる 捕手 クラフト 巫女\n",
      "\n",
      "Topic #1:\n",
      "いや バイファム 烏山 イスラエル クラス アルテ カタカナ ロエベ ジュリエット 提案 合意 こと プラズマ 刺客 大忙し 彝 法政大 シッポ ぼろ マツエク\n",
      "\n",
      "Topic #2:\n",
      "トレッサ リ 割り振り ドラゴンメダルゲット 崇 唐川 エリザベス 千本 ミスコン 味方 公演 保有 インドネシア 信 大津 喧噪 ライセンス どれ 大場 ヨルダン\n",
      "\n",
      "Topic #3:\n",
      "ロッソ かい お子様 イブニングキャンプ 件 横倒し コスメショップ 今 憤慨 ロンサム シアン パラリンピック 喪 林 エッセイ アワビ 刀剣 ヒミツ 水樹 アクセシビリティ\n",
      "\n",
      "Topic #4:\n",
      "シリアル ジャイアンツ 厳密 きんちょう 匹 フランクフルト 息 ロ ヴィブロス 本展 ゲル 山出 コング 次女 コマネラ シン 本来 ホット 日曜日 ジャケット\n",
      "\n",
      "Topic #5:\n",
      "先ほど 同時 挿し木 タワマン キャラメル エネルギッシュ 外交 歳児 払拭 ハリ 無残 がらみ 宿便 バランス キャップ 市村 ダシ シュンギク ペルセポネ 引換\n",
      "\n",
      "Topic #6:\n",
      "チェンジアップ 正樹 最小 亭 デスキャップ リナ ピッチ シンギュラリティ ギャル オン ミドル 基幹 わかりやすさ 大明 内勤 次代 存分 ジョイコン ファナティック 把\n",
      "\n",
      "Topic #7:\n",
      "フリッツ タイヤ ワカメ 日ハム 本気 模型 アダルト キャラクタ 想像 ぺい バナナ 単品 来店 新卒 寝床 廉価 レポ どきどき カ月 へん\n",
      "\n",
      "Topic #8:\n",
      "暖 拡張 健 ハイライト ヨット 周波 士 合流 グセ 手動 導入 彩 排便 メンテ 以内 ケン ブレス オダギリ カルテット 瀬戸内\n",
      "\n",
      "Topic #9:\n",
      "テニス エデル 東 欲求 恋情 密 セリフ 時点 多大 ロイヤリティ ゲノム 宮原 先ごろ 榎戸 ゾブリスト 低め ごっこ お寺 屈指 希望\n",
      "\n",
      "Topic #10:\n",
      "強権 杵 オリジナルバラエティ 大差 幸 ブレ 大体 後期 プリント ティッシュ 比呂志 ツルツルフェイス 創価大 幕張 幻惑 ひよこ 和気あいあい ナビタイムジャパン コミックアライブ マンガン\n",
      "\n",
      "Topic #11:\n",
      "まとまり ブリュンヒルデ ワイド モンハン ボトル 成 懐 推定 マイナス 塗り 俟 ひとり歩き 活字 民家 復旧 僕 アナログ イブ マテリアルズ リスペクト\n",
      "\n",
      "Topic #12:\n",
      "人為 みずほ ガラス 昭博 ピュア じまん クレムリン 圏外 ベア 岩手 ミラン エリック 機会 汗 ハガキ 旋波 さわやか ミヤネ 昨日 ずくめ\n",
      "\n",
      "Topic #13:\n",
      "ラクチン エンタメチャンネル アウト 多数 寿夫 の 夕 気楽 ヒサ カカオ シャンゼリゼ マガジンラック きもの おまかせ 女子 センバツ 存分 品位 前原 おうと\n",
      "\n",
      "Topic #14:\n",
      "ハウパ まな板 メタ 回数 エチゾ あっし プレイガイド 棚 斬新 沸点 ビルドアップ 夜曲 リオ ラジオ 告別 スペシャリティショップ 智恵子 カルビ ゼラニウム 始動\n",
      "\n",
      "Topic #15:\n",
      "濡れ エイシア ウッチャンナンチャン 崩壊 右京 小杉 判決 東宝 けた 残業 地元 従来 放出 ウェブメディア トランペット ゆか ブスタ カルテ 和樹 松原\n",
      "\n",
      "Topic #16:\n",
      "前払い スマホカメラマン 宝塚 整形 椅子 セキュア 渡海 リンゴタルト 焼きそば 妻 俵山 海流 戦況 成形 忠利 カプサイシン ゴジエヴァコラボ アングル トラディショナル マルチリンガル\n",
      "\n",
      "Topic #17:\n",
      "ヘッドマウントディスプレイ 増収 オシャレ マルカ ランチ キン 体温 柄もの 大声 武史 国政 ディラン 流動 ェクト 國 変形 みりん カイロ ネギ 尊\n",
      "\n",
      "Topic #18:\n",
      "成人 むじゃき 呼び出し ゼルダ 大技 時代 暴君 佳奈 平ら パッピンス アボガド コンプライアンス 没入 幕末 ビン 木曜日 活力 のれん すじ チャット\n",
      "\n",
      "Topic #19:\n",
      "居 気候 価値 コイン 太宰府天満宮 シンクロニシティ 外れ マニキュア セザンヌ プラス 本日 比例 ホログラムス カテキン わたくし ハワイ みどり 浩史 愉快 ゴシップ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# http://scikit-learn.org/stable/auto_examples/applications/\n",
    "# topics_extraction_with_nmf_lda.html　より\n",
    "#n_top_words = 20\n",
    "#for topic_idx, topic in enumerate(normalize_components):\n",
    "#    print('Topic #%d:' % topic_idx)\n",
    "#    print(' '.join([all_vocab_ar[i] for i in\n",
    "#                    topic.argsort()[:-n_top_words - 1:-1]]))#-1は逆順にindex番号をsortしている,-n_top_words - 1は列に関して\n",
    " #   print()\n",
    "\n",
    "n_top_words = 20\n",
    "for topic_idx, topic in enumerate(normalize_components):\n",
    "    print('Topic #%d:' % topic_idx)\n",
    "    print(' '.join([all_vocab_ar[i] for i in\n",
    "                    topic.argsort()[:-n_top_words-1:-1]]))#-1は逆順にindex番号をsortしている,-n_top_words - 1は列に関して\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文書 x トピック行列側も見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.49875062e-05, 2.49875062e-05, 2.49875062e-05, ...,\n",
       "        2.49875062e-05, 2.49875062e-05, 2.49875062e-05],\n",
       "       [3.83112405e-06, 3.83112405e-06, 3.83112405e-06, ...,\n",
       "        3.83112405e-06, 9.21757779e-02, 3.83112405e-06],\n",
       "       [1.47015584e-05, 1.47015584e-05, 1.47015584e-05, ...,\n",
       "        1.47015584e-05, 9.99720670e-01, 1.47015584e-05],\n",
       "       ...,\n",
       "       [4.95049505e-04, 4.95049505e-04, 4.95049505e-04, ...,\n",
       "        4.95049505e-04, 4.95049505e-04, 4.95049505e-04],\n",
       "       [1.51469252e-05, 1.51469252e-05, 1.51469252e-05, ...,\n",
       "        1.51469252e-05, 1.51469252e-05, 1.51469252e-05],\n",
       "       [1.99920032e-05, 1.99920032e-05, 1.99920032e-05, ...,\n",
       "        1.99920032e-05, 1.99920032e-05, 1.99920032e-05]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_data = model1.transform(A_train)\n",
    "doc_topic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learnのLDAはどうやら正規化されていないため、正規化した上で、1つ目の文書がどのトピックから来ている単語が多いのかを見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_doc_topic_data = \\\n",
    " doc_topic_data / doc_topic_data.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: probality: 0.000025\n",
      "Topic #1: probality: 0.000025\n",
      "Topic #2: probality: 0.000025\n",
      "Topic #3: probality: 0.000025\n",
      "Topic #4: probality: 0.000025\n",
      "Topic #5: probality: 0.000025\n",
      "Topic #6: probality: 0.000025\n",
      "Topic #7: probality: 0.000025\n",
      "Topic #8: probality: 0.000025\n",
      "Topic #9: probality: 0.000025\n",
      "Topic #10: probality: 0.000025\n",
      "Topic #11: probality: 0.999525\n",
      "Topic #12: probality: 0.000025\n",
      "Topic #13: probality: 0.000025\n",
      "Topic #14: probality: 0.000025\n",
      "Topic #15: probality: 0.000025\n",
      "Topic #16: probality: 0.000025\n",
      "Topic #17: probality: 0.000025\n",
      "Topic #18: probality: 0.000025\n",
      "Topic #19: probality: 0.000025\n"
     ]
    }
   ],
   "source": [
    "for topic_idx, prob in enumerate(normalize_doc_topic_data[0]):#[0]としているので1つ目の文書について\n",
    "    print('Topic #%d: probality: %f' % (topic_idx, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loglikelihood = model1.score(A_test)\n",
    "ppl = model1.perplexity(A_test)\n",
    "print('対数尤度: ', loglikelihood)\n",
    "print('Perplexity: ', ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストデータに当てはめてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc_topic_data = model1.transform(A_test)\n",
    "normalize_test_doc_topic_data = \\\n",
    " test_doc_topic_data / test_doc_topic_data.sum(axis=1, keepdims=True)\n",
    "for topic_idx, prob in enumerate(normalize_test_doc_topic_data[0]):\n",
    "    print('Topic #%d: probality: %f' % (topic_idx, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> LDAを適用してみよう (ldaパッケージを使った場合)</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = lda.LDA(n_topics=20, n_iter=1500, random_state=1, alpha=0.5, eta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model2.fit(A_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word = model2.topic_word_\n",
    "n_top_words = 20\n",
    "for topic_idx, topic in enumerate(topic_word):\n",
    "    print('Topic #%d:' % topic_idx)\n",
    "    print(' '.join([all_vocab_ar[i] for i in\n",
    "                    topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回も精度として対数尤度を見てみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.loglikelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文書 x トピック行列を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_data2 = model2.transform(A_train)\n",
    "for topic_idx, prob in enumerate(doc_topic_data2[0]):\n",
    "    print('Topic #%d: probality: %f' % (topic_idx, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>（参考）ディリクレ分布の挙動</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "K = 6\n",
    "sampled_probs = np.random.dirichlet([alpha for i in range(K)])\n",
    "for i, prob in enumerate(sampled_probs):\n",
    "    print('サイコロ %d面  確率: %.2f'%(i+1, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
